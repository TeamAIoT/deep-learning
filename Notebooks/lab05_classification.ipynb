{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab05_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CipT4UeP3AV-"
      },
      "source": [
        "# 데이터 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEkKYflTqMd3"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/TeamAIoT/deep-learning/main/Dataset/titanic_train.csv\n",
        "!wget https://raw.githubusercontent.com/TeamAIoT/deep-learning/main/Dataset/titanic_test.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3N-61rY3CgG"
      },
      "source": [
        "# 모듈 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81sccOFTF8Pj"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import Dataset,random_split,DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.cuda import is_available\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t56gGhCT3ERP"
      },
      "source": [
        "# GPU 사용을 위한 Device 세팅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWuNo3kStrqS"
      },
      "source": [
        "device='cuda' if is_available() else 'cpu'\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmdawh5E3IVY"
      },
      "source": [
        "# Titanic Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kmMPouu3KQu"
      },
      "source": [
        "## Titanic Dataset 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHftLPijp1n6"
      },
      "source": [
        "class TitanicDataset(Dataset):\n",
        "    def __init__(self,mode='train'):\n",
        "        super(TitanicDataset,self).__init__()\n",
        "        self.mode=mode\n",
        "        if self.mode=='train':\n",
        "            if os.path.exists('titanic_train.csv'):\n",
        "                self.data=pd.read_csv('titanic_train.csv')\n",
        "            else:\n",
        "                raise FileNotFoundError\n",
        "        elif self.mode=='test':\n",
        "            if os.path.exists('titanic_test.csv'):\n",
        "                self.data=pd.read_csv('titanic_test.csv')\n",
        "            else:\n",
        "                raise FileNotFoundError\n",
        "        else:\n",
        "            raise ValueError('Invaild argument at \\'mode\\'. expected \\'train\\' or \\'test\\'')\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        return torch.FloatTensor(self.data.iloc[idx,1:].values),torch.FloatTensor(self.data.iloc[idx,[0]].values)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7IMwlGD3M-t"
      },
      "source": [
        "## 학습을 위한 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHdDyQnRta2-"
      },
      "source": [
        "batch_size=32\n",
        "train_ratio=0.8\n",
        "\n",
        "all_data=TitanicDataset(mode='train')\n",
        "\n",
        "train_data_len=int(len(all_data)*0.8)\n",
        "valid_data_len=len(all_data)-train_data_len\n",
        "\n",
        "train_data,valid_data=random_split(all_data,[train_data_len,valid_data_len])\n",
        "\n",
        "train_loader=DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
        "valid_loader=DataLoader(valid_data,batch_size=batch_size,shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeCQP4pg3POH"
      },
      "source": [
        "## TitanicModel 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evzO3NRIsbS-"
      },
      "source": [
        "class TitanicModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TitanicModel,self).__init__()\n",
        "        self.layer1=nn.Linear(6,10)\n",
        "        self.layer2=nn.Linear(10,1)\n",
        "        self.activation=nn.Sigmoid()\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.layer1(x)\n",
        "        x=self.activation(x)\n",
        "        x=self.layer2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P188a9QA3Sbz"
      },
      "source": [
        "## 모델, 손실 함수, 최적화 알고리즘 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6CN4UxAtWRu"
      },
      "source": [
        "lr=0.001\n",
        "\n",
        "model=TitanicModel()\n",
        "criterion=nn.BCEWithLogitsLoss() # nn.Sigmoid() + nn.BCELoss()\n",
        "optimizer=optim.SGD(model.parameters(),lr=lr)\n",
        "\n",
        "model=model.to(device)\n",
        "criterion=criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTT36Txp3Vf7"
      },
      "source": [
        "## 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dPZD-RltPup"
      },
      "source": [
        "def train(model,criterion,optimizer,train_loader,valid_loader,num_epochs=10,print_every=1,early_stop=None,model_path='titanic.pth'):\n",
        "    train_logs={'Loss':[],'Accuracy':[]}\n",
        "    valid_logs={'Loss':[],'Accuracy':[]}\n",
        "    patience=0\n",
        "    best_acc=-np.inf\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss=0\n",
        "        valid_loss=0\n",
        "        train_acc=0\n",
        "        valid_acc=0\n",
        "        # training step\n",
        "        model.train()\n",
        "        for data,target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            data,target=data.to(device),target.to(device)\n",
        "            pred=model(data)\n",
        "            loss=criterion(pred,target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss+=loss.item()*data.size(0)\n",
        "            for p,t in zip(pred,target):\n",
        "                if p>=0 and t==1:\n",
        "                    train_acc+=1\n",
        "                elif p<0 and t==0:\n",
        "                    train_acc+=1\n",
        "        train_loss/=len(train_data)\n",
        "        train_acc/=len(train_data)\n",
        "        train_logs['Loss'].append(train_loss)\n",
        "        train_logs['Accuracy'].append(train_acc)\n",
        "        if (epoch+1)%print_every==0:\n",
        "            print('Training   Epoch {} - Loss : {:.8f} Accuracy : {:.4f}%'.format(epoch,train_loss,train_acc*100))\n",
        "        # validation step\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for data,target in valid_loader:\n",
        "                data,target=data.to(device),target.to(device)\n",
        "                pred=model(data)\n",
        "                loss=criterion(pred,target)\n",
        "                valid_loss+=loss.item()*data.size(0)\n",
        "                for p,t in zip(pred,target):\n",
        "                    if p>=0 and t==1:\n",
        "                        valid_acc+=1\n",
        "                    elif p<0 and t==0:\n",
        "                        valid_acc+=1\n",
        "            valid_loss/=len(valid_data)\n",
        "            valid_acc/=len(valid_data)\n",
        "            valid_logs['Loss'].append(valid_loss)\n",
        "            valid_logs['Accuracy'].append(valid_acc)\n",
        "            if (epoch+1)%print_every==0:\n",
        "                print('Validation Epoch {} - Loss : {:.8f} Accuracy : {:.4f}%'.format(epoch,valid_loss,valid_acc*100))\n",
        "            if valid_acc>best_acc:\n",
        "                best_acc=valid_acc\n",
        "                torch.save(model.state_dict(),model_path)\n",
        "                if early_stop is not None:\n",
        "                    patience=0\n",
        "            elif early_stop is not None:\n",
        "                patience+=1\n",
        "                if patience>=early_stop:\n",
        "                    print('Training finished by early stopping')\n",
        "                    return train_logs,valid_logs\n",
        "    return train_logs,valid_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD_UC1MiuDoN"
      },
      "source": [
        "train_logs,valid_logs = train(model=model,\n",
        "                              criterion=criterion,\n",
        "                              optimizer=optimizer,\n",
        "                              train_loader=train_loader,\n",
        "                              valid_loader=valid_loader,\n",
        "                              num_epochs=100,\n",
        "                              print_every=10,\n",
        "                              early_stop=None,\n",
        "                              model_path='titanic.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxUqTaWmQ7a-"
      },
      "source": [
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot([i for i in range(len(train_logs['Loss']))],train_logs['Loss'],label='train_loss')\n",
        "plt.plot([i for i in range(len(valid_logs['Loss']))],valid_logs['Loss'],label='valid_loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6iyfLNbRDMT"
      },
      "source": [
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot([i for i in range(len(train_logs['Accuracy']))],train_logs['Accuracy'],label='train_loss')\n",
        "plt.plot([i for i in range(len(valid_logs['Accuracy']))],valid_logs['Accuracy'],label='valid_loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J00kbQdC3Zil"
      },
      "source": [
        "## 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSQfPxeb1TNC"
      },
      "source": [
        "test_data=TitanicDataset(mode='test')\n",
        "test_loader=DataLoader(test_data,batch_size=1,shuffle=False)\n",
        "\n",
        "model.load_state_dict(torch.load('titanic.pth'))\n",
        "model=model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_lcd4Wp1dsq"
      },
      "source": [
        "def test(model,criterion,test_loader):\n",
        "    test_loss=0\n",
        "    test_acc=0\n",
        "    result_table=pd.DataFrame(columns=['Prediction','Target'])\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for data,target in test_loader:\n",
        "            data,target=data.to(device),target.to(device)\n",
        "            pred=model(data)\n",
        "            loss=criterion(pred,target)\n",
        "            test_loss+=loss.item()*data.size(0)\n",
        "            for p,t in zip(pred.view(-1),target.view(-1)):\n",
        "                if p>=0 and t==1:\n",
        "                    test_acc+=1\n",
        "                elif p<0 and t==0:\n",
        "                    test_acc+=1\n",
        "                result_table=result_table.append({'Prediction':1 if p>=0 else 0,'Target':t.item()},ignore_index=True)\n",
        "        test_loss/=len(test_data)\n",
        "        test_acc/=len(test_data)\n",
        "    return test_loss,test_acc,result_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02rKv9du1qUj"
      },
      "source": [
        "test_loss,test_acc,result_table=test(model=model,\n",
        "                                     criterion=criterion,\n",
        "                                     test_loader=test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v2DKcLU1tKq"
      },
      "source": [
        "print('Test Loss : {:.8f} Test Accuracy : {:.4f}%'.format(test_loss,test_acc*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y09s5j562lbZ"
      },
      "source": [
        "result_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCvhgCrwP6lL"
      },
      "source": [
        "# For entire comparison between prediction and target\n",
        "for i,(p,t) in enumerate(result_table.values):\n",
        "    print('{} - Pred: {} Target: {}'.format(i,p,t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeKUswvu3dhf"
      },
      "source": [
        "# MNIST Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r244fZk3rc7"
      },
      "source": [
        "## MNIST Dataset 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1rTz1853h2t"
      },
      "source": [
        "transform=transforms.ToTensor()\n",
        "\n",
        "all_data=MNIST(root='.',train=True,download=True,transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16zrEep44bQo"
      },
      "source": [
        "batch_size=128\n",
        "train_ratio=0.8\n",
        "\n",
        "train_data_len=int(len(all_data)*0.8)\n",
        "valid_data_len=len(all_data)-train_data_len\n",
        "\n",
        "train_data,valid_data=random_split(all_data,[train_data_len,valid_data_len])\n",
        "\n",
        "train_loader=DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
        "valid_loader=DataLoader(valid_data,batch_size=batch_size,shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18bhcnFi4fRb"
      },
      "source": [
        "## MNISTModel 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEd8_B6Y4ek5"
      },
      "source": [
        "class MNISTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTModel,self).__init__()\n",
        "        self.layer1=nn.Linear(28*28,1000)\n",
        "        self.layer2=nn.Linear(1000,10)\n",
        "        self.activation=nn.Sigmoid()\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=x.view(-1,28*28)\n",
        "        x=self.layer1(x)\n",
        "        x=self.activation(x)\n",
        "        x=self.layer2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU9N8nouSExx"
      },
      "source": [
        "## 모델, 손실 함수, 최적화 알고리즘 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q30vXGMT5l_R"
      },
      "source": [
        "lr=0.001\n",
        "\n",
        "model=MNISTModel()\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(),lr=lr)\n",
        "\n",
        "model=model.to(device)\n",
        "criterion=criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBRibyg0SIC7"
      },
      "source": [
        "## 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDDKKJl95rMe"
      },
      "source": [
        "def train(model,criterion,optimizer,train_loader,valid_loader,num_epochs=10,print_every=1,early_stop=None,model_path='mnist.pth'):\n",
        "    train_logs={'Loss':[],'Accuracy':[]}\n",
        "    valid_logs={'Loss':[],'Accuracy':[]}\n",
        "    patience=0\n",
        "    best_acc=-np.inf\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss=0\n",
        "        valid_loss=0\n",
        "        train_acc=0\n",
        "        valid_acc=0\n",
        "        # training step\n",
        "        model.train()\n",
        "        for data,target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            data,target=data.to(device),target.to(device)\n",
        "            pred=model(data)\n",
        "            loss=criterion(pred,target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss+=loss.item()*data.size(0)\n",
        "            train_acc+=torch.sum(pred.argmax(1)==target).item()\n",
        "        train_loss/=len(train_data)\n",
        "        train_acc/=len(train_data)\n",
        "        train_logs['Loss'].append(train_loss)\n",
        "        train_logs['Accuracy'].append(train_acc)\n",
        "        if (epoch+1)%print_every==0:\n",
        "            print('Training   Epoch {} - Loss : {:.8f} Accuracy : {:.4f}%'.format(epoch,train_loss,train_acc*100))\n",
        "        # validation step\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for data,target in valid_loader:\n",
        "                data,target=data.to(device),target.to(device)\n",
        "                pred=model(data)\n",
        "                loss=criterion(pred,target)\n",
        "                valid_loss+=loss.item()*data.size(0)\n",
        "                valid_acc+=torch.sum(pred.argmax(1)==target).item()\n",
        "            valid_loss/=len(valid_data)\n",
        "            valid_acc/=len(valid_data)\n",
        "            valid_logs['Loss'].append(valid_loss)\n",
        "            valid_logs['Accuracy'].append(valid_acc)\n",
        "            if (epoch+1)%print_every==0:\n",
        "                print('Validation Epoch {} - Loss : {:.8f} Accuracy : {:.4f}%'.format(epoch,valid_loss,valid_acc*100))\n",
        "            if valid_acc>best_acc:\n",
        "                best_acc=valid_acc\n",
        "                torch.save(model.state_dict(),model_path)\n",
        "                if early_stop is not None:\n",
        "                    patience=0\n",
        "            elif early_stop is not None:\n",
        "                patience+=1\n",
        "                if patience>=early_stop:\n",
        "                    print('Training finished by early stopping')\n",
        "                    return train_logs,valid_logs\n",
        "    return train_logs,valid_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NARzeG2V6GjF"
      },
      "source": [
        "train_logs,valid_logs = train(model=model,\n",
        "                              criterion=criterion,\n",
        "                              optimizer=optimizer,\n",
        "                              train_loader=train_loader,\n",
        "                              valid_loader=valid_loader,\n",
        "                              num_epochs=10,\n",
        "                              print_every=1,\n",
        "                              early_stop=None,\n",
        "                              model_path='mnist.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2xWboOsRSZ8"
      },
      "source": [
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot([i for i in range(len(train_logs['Loss']))],train_logs['Loss'],label='train_loss')\n",
        "plt.plot([i for i in range(len(valid_logs['Loss']))],valid_logs['Loss'],label='valid_loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1imxSQYoRS9t"
      },
      "source": [
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot([i for i in range(len(train_logs['Accuracy']))],train_logs['Accuracy'],label='train_loss')\n",
        "plt.plot([i for i in range(len(valid_logs['Accuracy']))],valid_logs['Accuracy'],label='valid_loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w6w4xeFSQDF"
      },
      "source": [
        "## 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra5uL6DdOAQL"
      },
      "source": [
        "test_data=MNIST(root='.',train=False,download=True,transform=transform)\n",
        "test_loader=DataLoader(test_data,batch_size=128,shuffle=False)\n",
        "\n",
        "model.load_state_dict(torch.load('mnist.pth'))\n",
        "model=model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WOYWR3-OxBR"
      },
      "source": [
        "def test(model,criterion,test_loader):\n",
        "    test_loss=0\n",
        "    test_acc=0\n",
        "    result_table=pd.DataFrame(columns=['Prediction','Target'])\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for data,target in test_loader:\n",
        "            data,target=data.to(device),target.to(device)\n",
        "            pred=model(data)\n",
        "            loss=criterion(pred,target)\n",
        "            test_loss+=loss.item()*data.size(0)\n",
        "            test_acc+=torch.sum(pred.argmax(1)==target).item()\n",
        "            for p,t in zip(pred.argmax(1),target):\n",
        "                result_table=result_table.append({'Prediction':p.item(),'Target':t.item()},ignore_index=True)\n",
        "        test_loss/=len(test_data)\n",
        "        test_acc/=len(test_data)\n",
        "    return test_loss,test_acc,result_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhnzcSGaPTAG"
      },
      "source": [
        "test_loss,test_acc,result_table=test(model=model,\n",
        "                                     criterion=criterion,\n",
        "                                     test_loader=test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEUgacTKPaER"
      },
      "source": [
        "print('Test Loss : {:.8f} Test Accuracy : {:.4f}%'.format(test_loss,test_acc*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVivjzyyPgCg"
      },
      "source": [
        "result_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSJtYovuP1rn"
      },
      "source": [
        "# For entire comparison between prediction and target\n",
        "for i,(p,t) in enumerate(result_table.values):\n",
        "    print('{} - Pred: {} Target: {}'.format(i,p,t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxZJdHFXR1Vb"
      },
      "source": [
        "## 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4BVCZuAP_B0"
      },
      "source": [
        "tensorToImage=transforms.ToPILImage()\n",
        "fig=plt.figure(figsize=(10,12))\n",
        "cols=4\n",
        "rows=4\n",
        "sample_index=np.random.randint(0,len(test_data),size=(cols*rows))\n",
        "\n",
        "for i in range(1,cols*rows+1):\n",
        "    img=tensorToImage(test_data[sample_index[i-1]][0])\n",
        "    pred=model(test_data[sample_index[i-1]][0].unsqueeze(0).to(device))\n",
        "    fig.add_subplot(rows,cols,i)\n",
        "    plt.title('pred:{} label:{}'.format(pred.argmax().cpu().item(),test_data[sample_index[i-1]][1]))\n",
        "    plt.imshow(img,cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}