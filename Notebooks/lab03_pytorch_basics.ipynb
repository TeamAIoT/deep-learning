{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab03_pytorch_basics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UAz42LoF4hM"
      },
      "source": [
        "# Tensor Manipulation\n",
        "1. Numpy  \n",
        "PyTorch는 Numpy에서 배열을 다루는 것과 유사하게 텐서를 다룹니다.  \n",
        "PyTorch를 시작하기에 앞서 Numpy에서는 어떻게 배열을 다루는 지 살펴보도록 합시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SnSaf09FSLc"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igO3ehRBF1P2"
      },
      "source": [
        "a=np.array([1,2,3,4,5,6,7,8])\n",
        "\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFLjS4HeGkM6"
      },
      "source": [
        "b=np.array([[1,2,3,4],[5,6,7,8]])\n",
        "\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km_Ij1LoHDtV"
      },
      "source": [
        "c=np.array([[[1,2],[3,4]],[[5,6],[7,8]]])\n",
        "\n",
        "print(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWk61UaAH92C"
      },
      "source": [
        "print('a:',a)\n",
        "print('a.ndim:',a.ndim)\n",
        "print('a.shape:',a.shape)\n",
        "\n",
        "print()\n",
        "\n",
        "print('b:',b)\n",
        "print('b.ndim:',b.ndim)\n",
        "print('b.shape:',b.shape)\n",
        "\n",
        "print()\n",
        "\n",
        "print('c:',c)\n",
        "print('c.ndim:',c.ndim)\n",
        "print('c.shape:',c.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOQ8tLcSGMic"
      },
      "source": [
        "2. PyTorch  \n",
        "PyTorch에서도 numpy에서와 비슷하게 텐서를 다룰 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeaY13tnOaLK"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvNA8mPYObxh"
      },
      "source": [
        "a=torch.Tensor([1,2,3,4,5,6,7,8])\n",
        "\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA3fcfXROhdJ"
      },
      "source": [
        "b=torch.Tensor([[1,2,3,4],[5,6,7,8]])\n",
        "\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE3qce8FPoPq"
      },
      "source": [
        "c=torch.Tensor([[[1,2],[3,4]],[[5,6],[7,8]]])\n",
        "\n",
        "print(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jQ1H5oMAZpx"
      },
      "source": [
        "print('a:',a)\n",
        "print('a.ndim:',a.ndim)\n",
        "print('a.shape:',a.shape)\n",
        "\n",
        "print()\n",
        "\n",
        "print('b:',b)\n",
        "print('b.ndim:',b.ndim)\n",
        "print('b.shape:',b.shape)\n",
        "\n",
        "print()\n",
        "\n",
        "print('c:',c)\n",
        "print('c.ndim:',c.ndim)\n",
        "print('c.shape:',c.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxdHnwXVVyJU"
      },
      "source": [
        "3. Tensor Operations  \n",
        "이번에는 텐서 간의 연산을 다뤄 보도록 하겠습니다.  \n",
        "PyTorch에서도 많은 연산들을 지원하지만 여기서는 간단하게 다음 5가지 연산을 다룰 예정입니다.  \n",
        "    - 합\n",
        "    - 차\n",
        "    - 스칼라 배\n",
        "    - 원소 간 곱\n",
        "    - 행렬 곱   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOIZilIRDTAD"
      },
      "source": [
        "A=torch.Tensor([[1,2],[3,4]])\n",
        "B=torch.Tensor([[3,4],[5,6]])\n",
        "\n",
        "print('\\nAdd')\n",
        "print(A+B)\n",
        "\n",
        "print('\\nSubtract')\n",
        "print(A-B)\n",
        "\n",
        "print('\\nScalar Multiplication')\n",
        "print(3*A)\n",
        "\n",
        "print('\\nElement-wise Multiplication')\n",
        "print(A*B)\n",
        "print(A.mul(B))\n",
        "\n",
        "print('\\nMatrix multiplication')\n",
        "print(A.matmul(B))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgImWv1NXNQy"
      },
      "source": [
        "4. Statistical Function  \n",
        "    이번에는 각종 통계 함수들을 다뤄 볼 예정입니다.  \n",
        "    통계 함수를 그냥 사용하게 되면 텐서 전체에 대한 결과가 반환됩니다.  \n",
        "    통계 함수들에 dim 인자를 주게 되면, 해당 차원을 제외한 텐서에서  \n",
        "    통계 함수의 결과가 반환됩니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHXppmGENZqU"
      },
      "source": [
        "print(A)\n",
        "\n",
        "print('\\nMean')\n",
        "print(A.mean())\n",
        "print(A.mean(dim=0))\n",
        "print(A.mean(dim=1))\n",
        "\n",
        "print('\\nSum')\n",
        "print(A.sum())\n",
        "print(A.sum(dim=0))\n",
        "print(A.sum(dim=1))\n",
        "\n",
        "print('\\nMin')\n",
        "print(A.min())\n",
        "print(A.min(dim=0))\n",
        "print(A.min(dim=1))\n",
        "\n",
        "print('\\nMax')\n",
        "print(A.max())\n",
        "print(A.max(dim=0))\n",
        "print(A.max(dim=1))\n",
        "\n",
        "print('\\nArgmin')\n",
        "print(A.argmin())\n",
        "print(A.argmin(dim=0))\n",
        "print(A.argmin(dim=1))\n",
        "\n",
        "print('\\nArgmax')\n",
        "print(A.argmax())\n",
        "print(A.argmax(dim=0))\n",
        "print(A.argmax(dim=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1wcp7J2YAQt"
      },
      "source": [
        "5. Tensor Shape Manipulation  \n",
        "    이번에는 view(), squeeze(), unsqueeze()를 통해 텐서의 모양을 조작해 보도록 하겠습니다.  \n",
        "    - view()  \n",
        "        view()는 텐서의 shape를 입력받아 해당 shape로 텐서의 모양을 변경합니다.  \n",
        "        -1을 입력할 수 도 있는데, 이 경우 -1로 입력된 부분은 자동으로 계산됩니다.\n",
        "\n",
        "    - squeeze()  \n",
        "        텐서에서 크기가 1인 차원을 압축하여 제거합니다.\n",
        "\n",
        "    - unsqueeze()  \n",
        "        인자 dim에 해당하는 위치에 크기가 1인 차원을 추가합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j67QPaA2N-BH"
      },
      "source": [
        "print(A.view(4))\n",
        "print(A.view(4).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "598blpBSQms7"
      },
      "source": [
        "print(A.view(4,1))\n",
        "print(A.view(4,1).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfjoh3skQyaq"
      },
      "source": [
        "print(A.view(1,4))\n",
        "print(A.view(1,4).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DEJelhMQ7f2"
      },
      "source": [
        "print(A.view(1,-1))\n",
        "print(A.view(1,-1).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvOw1sq8S7SO"
      },
      "source": [
        "B=A.view(1,2,2,1)\n",
        "\n",
        "print(B.shape)\n",
        "\n",
        "print(B.squeeze())\n",
        "print(B.squeeze().shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nTcvybHUhKz"
      },
      "source": [
        "print(A.unsqueeze(0).shape)\n",
        "print(A.unsqueeze(1).shape)\n",
        "print(A.unsqueeze(1).unsqueeze(0).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlCX4X55v6hH"
      },
      "source": [
        "# torch.nn  \n",
        "torch.nn은 모델을 구성하기 위한 다양한 함수, 활성 함수, 손실 함수 등이 구현되어있는 라이브러리이다.  \n",
        "  \n",
        "보통 torch.nn.Module을 상속하여 사용한다.  \n",
        "새로 구현한 클래스는 \\__init__\\()과 forward()를 모델에 맞게 구현해주어야한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfTPdQ6Pl3mE"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Egr2JeYYCoo"
      },
      "source": [
        "class MyModule(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyModule,self).__init__()\n",
        "    self.W=torch.Tensor([1])\n",
        "    self.b=torch.Tensor([1])\n",
        "    \n",
        "  def forward(self,x):\n",
        "    return self.W*x+self.b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av1VWdqEYKVL"
      },
      "source": [
        "model=MyModule()\n",
        "\n",
        "y=model(1) # forward() 실행\n",
        "\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIkFjK5V4qAl"
      },
      "source": [
        "- nn.Linear(in_features, out_features,...)  \n",
        "nn.Linear는 선형 대수학의 선형 변환에 해당하는 layer로  \n",
        "간단히 얘기해서 다항식 또는 퍼셉트론을 구현한 것이라고 이해하면 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXmO2cENl_MZ"
      },
      "source": [
        "layer1=nn.Linear(2,3)\n",
        "\n",
        "x=torch.Tensor(2)\n",
        "y=layer1(x)\n",
        "\n",
        "print('Input shape:',x.shape)\n",
        "print('Output shape:',y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ActT7kG-5npb"
      },
      "source": [
        "보통은 아래와 같이 클래스를 구현하여 사용한다.  \n",
        "print()를 이용하면 모델의 구성을 파악할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9j0Or0CsOi4"
      },
      "source": [
        "class LinearModel(nn.Module):\n",
        "    def __init__(self,in_features,out_features):\n",
        "        super(LinearModel,self).__init__()\n",
        "        self.layer1=nn.Linear(in_features,out_features)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.layer1(x)\n",
        "        return x\n",
        "\n",
        "model=LinearModel(1,10)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjuDYesI8vlP"
      },
      "source": [
        "여러 layer를 사용하는 경우 다음과 같이 구현할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JTeLZ6svGAN"
      },
      "source": [
        "class SequentialModel(nn.Module):\n",
        "    def __init__(self,in_features,out_features):\n",
        "        super(SequentialModel,self).__init__()\n",
        "        self.layer1=nn.Linear(in_features,out_features)\n",
        "        self.sigmoid=nn.Sigmoid()\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.layer1(x)\n",
        "        x=self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "model=SequentialModel(1,10)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCkEBQEt8z1E"
      },
      "source": [
        "각 layer들이 순차적으로 구성되어있다면 아래와 같이  \n",
        "nn.Sequential()을 이용하면 보다 편리하게 구현할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zouajx_TvgvE"
      },
      "source": [
        "class SequentialModel2(nn.Module):\n",
        "    def __init__(self,in_features,out_features):\n",
        "        super(SequentialModel2,self).__init__()\n",
        "        self.layers=nn.Sequential(\n",
        "            nn.Linear(in_features,out_features),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.layers(x)\n",
        "\n",
        "model=SequentialModel2(1,10)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUZpWO6VAA6F"
      },
      "source": [
        "torch.nn을 이용하여 모델을 정의하고 데이터를 모델의 입력으로 주는 과정은 다음과 같습니다.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3srTkyts7yG"
      },
      "source": [
        "train_x=torch.Tensor([[1],[2],[3],[4]])\n",
        "train_y=torch.Tensor([[2],[3],[4],[5]])\n",
        "\n",
        "model=LinearModel(1,1)\n",
        "criterion=nn.MSELoss()\n",
        "\n",
        "pred=model(train_x)\n",
        "loss=criterion(pred,train_y)\n",
        "\n",
        "print('input shape:',train_x.shape)\n",
        "print('output shape:',pred.shape)\n",
        "\n",
        "print('pred:',pred)\n",
        "print('loss:',loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUAJ1rA1ip3J"
      },
      "source": [
        "# torch.optim  \n",
        "torch.optim은 pytorch의 매개 변수 최적화 알고리즘이 구현된 라이브러리입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3qgBwMRj9HF"
      },
      "source": [
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_mwtOuPTrvv"
      },
      "source": [
        "바로 이전 코드의 LinearModel을 torch.optim.SGD를 이용하여 학습시켜 보도록 하겠습니다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKj76kBASH4D"
      },
      "source": [
        "train_x=torch.Tensor([[1],[2],[3],[4]])\n",
        "train_y=torch.Tensor([[2],[3],[4],[5]])\n",
        "\n",
        "model=LinearModel(1,1)\n",
        "criterion=nn.MSELoss()\n",
        "\n",
        "pred=model(train_x)\n",
        "loss=criterion(pred,train_y)\n",
        "optimizer=optim.SGD(model.parameters(),lr=0.001)\n",
        "\n",
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    pred=model(train_x)\n",
        "    loss=criterion(pred,train_y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch%100==0:\n",
        "        print('Epoch {}:\\tpred: {}\\tloss: {}'.format(epoch,pred.view(-1).detach().numpy(),loss))\n",
        "        # detach()를 사용하게 되면 더이상 그래디언트를 계산시키지 않는다는 의미\n",
        "        # numpy()로 변환하기 위해서는 detach()가 선행되어야함"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6-mvQObWuQp"
      },
      "source": [
        "# torch.utils.data  \n",
        "torch.utils.data에는 pytorch에서 데이터를 다루기 위한 유틸리티들이 구현되어있습니다.  \n",
        "이 중 `Dataset`,`random_split`,`DataLoader`를 다뤄보도록 하겠습니다.  \n",
        "이번에는 사인파(sine wave)를 예측해보도록 하겠습니다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VONXaY12gadc"
      },
      "source": [
        "from torch.utils.data import Dataset, random_split, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeneKdengpRA"
      },
      "source": [
        "1. Dataset  \n",
        "사용자는 보통 Dataset 클래스를 상속하여 문제에 맞는 사용자 정의 클래스를 구현합니다.  \n",
        "`__init__`,`__getitem__`,`__len__`을 구현하여 오버라이딩합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhGPM1bIgkGu"
      },
      "source": [
        "class SineDataset(Dataset):\n",
        "    def __init__(self,min_bound=-10,max_bound=10,step=0.1):\n",
        "        self.x=np.arange(min_bound,max_bound,step)\n",
        "        self.y=np.sin(self.x)\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        return torch.FloatTensor([self.x[idx]]),torch.FloatTensor([self.y[idx]])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x) # return len(self.y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjLATZ8-jSgy"
      },
      "source": [
        "all_data=SineDataset(-5,5)\n",
        "\n",
        "print('x:',all_data.x)\n",
        "print('y:',all_data.y)\n",
        "print('length:',len(all_data))\n",
        "x0,y0=all_data[0]\n",
        "print('x[0],y[0]:',x0,y0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq6NGbuWjuV7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwApMcWjjxzy"
      },
      "source": [
        "plt.plot(all_data.x,all_data.y,all_data.x,[0 for i in range(len(all_data))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URHTXh1AkRFI"
      },
      "source": [
        "2. random_split  \n",
        "random_split은 pytorch의 데이터셋을 분할시켜주는 함수입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY8Uif-tkXbp"
      },
      "source": [
        "train_ratio=0.8\n",
        "\n",
        "train_data_len=int(len(all_data)*0.8)\n",
        "valid_data_len=len(all_data)-train_data_len\n",
        "\n",
        "train_data,valid_data=random_split(all_data,[train_data_len,valid_data_len])\n",
        "\n",
        "print('train size:',len(train_data))\n",
        "print('valid size:',len(valid_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCJJc4xFktyg"
      },
      "source": [
        "3. DataLoader  \n",
        "DataLoader는 학습에 사용하기 위해 데이터를 불러오는 데 사용되는 클래스입니다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2Fj-9-Jktgf"
      },
      "source": [
        "batch_size=16\n",
        "\n",
        "train_loader=DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
        "valid_loader=DataLoader(valid_data,batch_size=batch_size,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCokQAPilEFo"
      },
      "source": [
        "이전에 구현했던 LinearModel을 이용해 사인파 데이터를 학습시키고  \n",
        "검증 절차까지 진행해 보도록 하겠습니다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrVtTPkHlVs6"
      },
      "source": [
        "model=LinearModel(1,1)\n",
        "criterion=nn.MSELoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwaKFN1-livx"
      },
      "source": [
        "num_epochs=10000\n",
        "print_every=1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss=0\n",
        "    for data,target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred=model(data)\n",
        "        loss=criterion(pred,target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss+=loss.item()*data.size(0)\n",
        "    train_loss/=len(train_data)\n",
        "    if epoch%print_every==0:\n",
        "        print('Train Epoch {} - Loss : {:.6f}'.format(epoch,train_loss))\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        valid_loss=0\n",
        "        for data,target in valid_loader:\n",
        "            pred=model(data)\n",
        "            loss=criterion(pred,target)\n",
        "            valid_loss+=loss.item()*data.size(0)\n",
        "        valid_loss/=len(valid_data)\n",
        "        if epoch%print_every==0:\n",
        "            print('Validation Epoch {} - Loss : {:.6f}'.format(epoch,valid_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QPdy5DOoeTD"
      },
      "source": [
        "test_loader=DataLoader(all_data,batch_size=batch_size,shuffle=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_loss=0\n",
        "    preds=[]\n",
        "    for data,target in test_loader:\n",
        "        pred=model(data)\n",
        "        loss=criterion(pred,target)\n",
        "        preds.extend(pred.view(-1).numpy())\n",
        "        test_loss+=loss.item()*data.size(0)\n",
        "    test_loss/=len(all_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YrIOgQvqUWz"
      },
      "source": [
        "print('test loss:',test_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovVe1CzHq6ZO"
      },
      "source": [
        "plt.plot(all_data.x,all_data.y,all_data.x,preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afL7l_6YlIe2"
      },
      "source": [
        "위 그래프는 기존 사인파(파란색)과 모델의 예측값(주황색)을 비교한 자료입니다.  \n",
        "\n",
        "`LinearModel`의 경우 모델의 크기가 작아 데이터를 정확하게 표현하지 못하는 언더피팅이 발생했습니다.  \n",
        "\n",
        "이에 `LinearModel`보다 다소 매개변수가 늘어난 `Model`을 정의해 보겠습니다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QTCHlMhrNtp"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self,in_features,out_features):\n",
        "        super(Model,self).__init__()\n",
        "        self.layers=nn.Sequential(\n",
        "            nn.Linear(in_features,10),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(10,10),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(10,out_features)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.layers(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJUW1VXzruHP"
      },
      "source": [
        "model=Model(1,1)\n",
        "criterion=nn.MSELoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr370vp6lwkQ"
      },
      "source": [
        "num_epochs=10000\n",
        "print_every=1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss=0\n",
        "    for data,target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred=model(data)\n",
        "        loss=criterion(pred,target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss+=loss.item()*data.size(0)\n",
        "    train_loss/=len(train_data)\n",
        "    if epoch%print_every==0:\n",
        "        print('Train Epoch {} - Loss : {:.6f}'.format(epoch,train_loss))\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        valid_loss=0\n",
        "        for data,target in valid_loader:\n",
        "            pred=model(data)\n",
        "            loss=criterion(pred,target)\n",
        "            valid_loss+=loss.item()*data.size(0)\n",
        "        valid_loss/=len(valid_data)\n",
        "        if epoch%print_every==0:\n",
        "            print('Validation Epoch {} - Loss : {:.6f}'.format(epoch,valid_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QDoolZcmEtR"
      },
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_loss=0\n",
        "    preds=[]\n",
        "    for data,target in test_loader:\n",
        "        pred=model(data)\n",
        "        loss=criterion(pred,target)\n",
        "        preds.extend(pred.view(-1).numpy())\n",
        "        test_loss+=loss.item()*data.size(0)\n",
        "    test_loss/=len(all_data)\n",
        "\n",
        "print('test loss:',test_loss)\n",
        "\n",
        "plt.plot(all_data.x,all_data.y,all_data.x,preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLp4GXMOmOPo"
      },
      "source": [
        "모델의 용량이 증가하여 표현 능력이 향상된 것을 확인할 수 있습니다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSrWku6Xm5KE"
      },
      "source": [
        "# 모델 저장하기 & 불러오기  \n",
        "모델의 저장은 `nn.Module.state_dict()`와 `torch.save()`를 이용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC3YiSOinVxj"
      },
      "source": [
        "model=LinearModel(1,1)\n",
        "\n",
        "print(model.state_dict())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpVWbrhSnwxo"
      },
      "source": [
        "optimizer=optim.SGD(model.parameters(),lr=0.0001)\n",
        "\n",
        "print(optimizer.state_dict())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o-BN509pHUm"
      },
      "source": [
        "model_path='./model.pth'\n",
        "\n",
        "torch.save(model.state_dict(),model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIj7GlIOrZ0s"
      },
      "source": [
        "모델을 불러오는 과정은 `torch.load()`와 `nn.Module.load_state_dict()`를 사용."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3ICEbjcqNwu"
      },
      "source": [
        "model.load_state_dict(torch.load(model_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVGknRbrs9nL"
      },
      "source": [
        "모델을 저장하고 불러오기가 가능해 지면  \n",
        "early stopping을 학습에 적용할 수 있습니다.  \n",
        "  \n",
        "Early Stopping은 patience를 설정하여,  \n",
        "validation loss가 patience 만큼의 epoch동안 개선되지 않으면  \n",
        "over fitting이라고 간주하고 학습을 종료하는 것 입니다.  \n",
        "\n",
        "모델의 저장, 불러오기와 early stopping을 적용한 학습을 진행시켜 보겠습니다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUEnBsTVtll-"
      },
      "source": [
        "model=Model(1,1)\n",
        "criterion=nn.MSELoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASRZi3FBtpR2"
      },
      "source": [
        "num_epochs=10000\n",
        "print_every=1000\n",
        "patience=0\n",
        "min_loss=np.inf\n",
        "early_stop=75\n",
        "model_path='./model.pth'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss=0\n",
        "    for data,target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred=model(data)\n",
        "        loss=criterion(pred,target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss+=loss.item()*data.size(0)\n",
        "    train_loss/=len(train_data)\n",
        "    if epoch%print_every==0:\n",
        "        print('Train Epoch {} - Loss : {:.6f}'.format(epoch,train_loss))\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        valid_loss=0\n",
        "        for data,target in valid_loader:\n",
        "            pred=model(data)\n",
        "            loss=criterion(pred,target)\n",
        "            valid_loss+=loss.item()*data.size(0)\n",
        "        valid_loss/=len(valid_data)\n",
        "        if epoch%print_every==0:\n",
        "            print('Validation Epoch {} - Loss : {:.6f}'.format(epoch,valid_loss))\n",
        "        if valid_loss<min_loss:\n",
        "            patience=0\n",
        "            torch.save(model.state_dict(),model_path)\n",
        "            min_loss=valid_loss\n",
        "        else:\n",
        "            patience+=1\n",
        "        if patience==early_stop:\n",
        "            print('Training finished by early stopping')\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y13AvGniupVl"
      },
      "source": [
        "model.load_state_dict(torch.load(model_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE0c8_M-uwAL"
      },
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_loss=0\n",
        "    preds=[]\n",
        "    for data,target in test_loader:\n",
        "        pred=model(data)\n",
        "        loss=criterion(pred,target)\n",
        "        preds.extend(pred.view(-1).numpy())\n",
        "        test_loss+=loss.item()*data.size(0)\n",
        "    test_loss/=len(all_data)\n",
        "\n",
        "print('test loss:',test_loss)\n",
        "\n",
        "plt.plot(all_data.x,all_data.y,all_data.x,preds)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}